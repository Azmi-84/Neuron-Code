{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div><div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview and Imports\n",
    "#Recurrent Neural Networks (RNNs) are a type of neural network that are designed to handle sequential data, such as time series or natural language text. They achieve this by using a hidden state that is updated for each time step of the input sequence, allowing the network to maintain a memory of previous inputs.\n",
    " \n",
    "\n",
    "#This notebook contains an implementation of an RNN that can be used for language modeling. The self takes in a sequence of characters and outputs the probability distribution over the next character in the sequence. The network is trained on a corpus of text and then used to generate new text that has a similar distribution of characters as the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation \n",
    "\n",
    "class DataGenerator:\n",
    "    \"\"\"\n",
    "    A class for generating input and output examples for a character-level RNN.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, path):\n",
    "        \"\"\"Initiate the DataGenerator object.\n",
    "\n",
    "        Args:\n",
    "            path (str): The path to the text file containing the data.\n",
    "        \"\"\"\n",
    "        self.path = path\n",
    "        \n",
    "        # Read the data from the file and convert to the lower case\n",
    "        with open(path) as f:\n",
    "            self.data = f.read().lower()\n",
    "        \n",
    "        # Create an list of unique characters in the data\n",
    "        self.chars = list(set(self.data))\n",
    "        \n",
    "        # Create a dictionary mapping characters to their indices in the list of unique characters\n",
    "        self.char_to_index = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.index_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        # Set the size of the vocabulary (i.e. the number of unique characters)\n",
    "        self.vocab_size = len(self.chars)\n",
    "        \n",
    "        # Read in examples from file and convert to lowercase, removing leading and trailing white space\n",
    "        with open(path) as f:\n",
    "            examples = f.readlines()\n",
    "            self.examples = [example.strip().lower() for example in examples]\n",
    "            \n",
    "    def generate_example(self):\n",
    "        \"\"\"Generate a random example from the data.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the input and output examples.\n",
    "        \"\"\"\n",
    "        # Select a random example from the list of examples\n",
    "        example_chars = self.examples[idx]\n",
    "        \n",
    "        # Convert the characters in the examples to their corresponding indices in the list of unique characters\n",
    "        example_char_indices = [self.char_to_index[ch] for ch in example_chars]\n",
    "        \n",
    "        # Add a newline character as the first character of the input array, and as the last character in the output array\n",
    "        \n",
    "        X = [self.char_to_index['\\n']] + example_char_indices\n",
    "        Y = example_char_indices + [self.char_to_index['\\n']]\n",
    "        \n",
    "        return np.array(X), np.array(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN  Implementation <a name=\"3-1\"></a>\n",
    "**The RNN used in this notebook is a basic one-layer RNN. It consists of an input layer, a hidden layer, and an output layer. The input layer takes in a one-hot encoded vector representing a character in the input sequence. This vector is multiplied by a weight matrix  $W_{ax}$ to produce a hidden state vector $a$. The hidden state vector is then passed through a non-linear activation function (in this case, the hyperbolic tangent function) and updated for each time step of the input sequence. The updated hidden state is then multiplied by a weight matrix  $W_{ya}$ to produce the output probability distribution over the next character in the sequence.**\n",
    "\n",
    "**The RNN is trained using stochastic gradient descent with the cross-entropy loss function. During training, the self takes in a sequence of characters and outputs the probability distribution over the next character. The true next character is then compared to the predicted probability distribution, and the parameters of the network are updated to minimize the cross-entropy loss.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation Functions\n",
    "### Softmax Activation Function\n",
    "\n",
    "**$$\\mathrm{softmax}(\\mathbf{x})_i = \\frac{e^{x_i}}{\\sum_{j=1}^n e^{x_j}}$$**\n",
    "\n",
    "**The softmax function is commonly used as an activation function in neural networks, particularly in the output layer for classification tasks. Given an input array $x$, the softmax function calculates the probability distribution of each element in the array**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### Tanh Activation\n",
    "**$$\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$$**\n",
    "\n",
    "**where $x$ is the input to the function. The output of the function is a value between -1 and 1. The tanh activation function is often used in neural networks as an alternative to the sigmoid activation function, as it has a steeper gradient and can better model non-linear relationships in the data.**\n",
    "****\n",
    "\n",
    "## Forward propagation:\n",
    "\n",
    "**During forward propagation, the input sequence is processed through the RNN to generate an output sequence. At each time step, the hidden state and the output are computed using the input, the previous hidden state, and the RNN's parameters.**\n",
    "\n",
    "**The equations for the forward propagation in a basic RNN are as follows:**\n",
    "\n",
    "**At time step $t$, the input to the RNN is $x_t$, and the hidden state at time step $t-1$ is $a_{t-1}$. The hidden state at time step $t$ is computed as:**\n",
    "\n",
    "**$a_t = \\tanh(W_{aa} a_{t-1} + W_{ax} x_t + b_a)$**\n",
    "\n",
    "**where $W_{aa}$ is the weight matrix for the hidden state, $W_{ax}$ is the weight matrix for the input, and $b_a$ is the bias vector for the hidden state.**\n",
    "\n",
    "**The output at time step $t$ is computed as:**\n",
    "\n",
    "**$y_t = softmax(W_{ya} a_t + b_y)$**\n",
    "\n",
    "**where $W_{ya}$ is the weight matrix for the output, and $b_y$ is the bias vector for the output.**\n",
    "****\n",
    "## Backward propagation:\n",
    "\n",
    "**The objective of training an RNN is to minimize the loss between the predicted sequence and the ground truth sequence. Backward propagation calculates the gradients of the loss with respect to the RNN's parameters, which are then used to update the parameters using an optimization algorithm such as Adagrad or Adam.**\n",
    "\n",
    "**The equations for the backward propagation in a basic RNN are as follows:**\n",
    "\n",
    "**At time step $t$, the loss with respect to the output $y_t$ is given by:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial y_t} = -\\frac{1}{y_{t,i}} \\text{ if } i=t_i, \\text{ else } 0$**\n",
    "\n",
    "**where $L$ is the loss function, $y_{t,i}$ is the $i$th element of the output at time step $t$, and $t_i$ is the index of the true label at time step $t$**.\n",
    "\n",
    "**The loss with respect to the hidden state at time step $t$ is given by:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial a_t} = \\frac{\\partial L}{\\partial y_t} W_{ya} + \\frac{\\partial L}{\\partial h_{t+1}} W_{aa}$**\n",
    "\n",
    "**where $\\frac{\\partial L}{\\partial a_{t+1}}$ is the gradient of the loss with respect to the hidden state at the next time step, which is backpropagated through time.**\n",
    "\n",
    "**The gradient with respect to tanh is given by:**\n",
    "**$\\frac{\\partial \\tanh(a)} {\\partial a}$**\n",
    "\n",
    "**The gradients with respect to the parameters are then computed using the chain rule:**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{ya}} = \\sum_t \\frac{\\partial L}{\\partial y_t} a_t$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial b_y} = \\sum_t \\frac{\\partial L}{\\partial y_t}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{ax}} = \\sum_t \\frac{\\partial L}{\\partial a_t} \\frac{\\partial a_t}{\\partial W_{ax}}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial W_{aa}} = \\sum_t \\frac{\\partial L}{\\partial h_t} \\frac{\\partial h_t}{\\partial W_{aa}}$**\n",
    "\n",
    "**$\\frac{\\partial L}{\\partial b_a} = \\sum_t \\frac{\\partial L}{\\partial a_t} \\frac{\\partial h_t}{\\partial b_a}$**\n",
    "\n",
    "**where $\\frac{\\partial h_t}{\\partial W_{ax}}$, $\\frac{\\partial a_t}{\\partial W_{aa}}$, and $\\frac{\\partial h_t}{\\partial b_a}$ can be computed as:**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial W_{ax}} = x_t$**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial W_{aa}} = a_{t-1}$**\n",
    "\n",
    "**$\\frac{\\partial a_t}{\\partial b_a} = 1$**\n",
    "\n",
    "**These gradients are then used to update the parameters of the RNN using an optimization algorithm such as gradient descent, Adagrad, or Adam.**\n",
    "****\n",
    "## Loss:\n",
    "\n",
    "**The cross-entropy loss between the predicted probabilities y_pred and the true targets y_true at a single time step $t$ is:**\n",
    "\n",
    "**$$H(y_{true,t}, y_{pred,t}) = -\\sum_i y_{true,t,i} \\log(y_{pred,t,i})$$**\n",
    "\n",
    "**where $y_{pred,t}$ is the predicted probability distribution at time step $t$, $y_{true,t}$ is the true probability distribution at time step $t$ (i.e., a one-hot encoded vector representing the true target), and $i$ ranges over the vocabulary size.**\n",
    "\n",
    "**The total loss is then computed as the sum of the cross-entropy losses over all time steps:**\n",
    "\n",
    "**$$L = \\sum_{t=1}^{T} H(y_{true,t}, y_{pred,t})$$**\n",
    "\n",
    "**where $T$ is the sequence length.**\n",
    " \n",
    "****\n",
    "\n",
    "## Train:\n",
    "**The train method trains the RNN on a dataset using backpropagation through time. The method takes an instance of DataReader containing the training data as input. The method initializes a hidden state vector a_prev at the beginning of each sequence to zero. It then iterates until the smooth loss is less than a threshold value.**\n",
    "\n",
    "**During each iteration, it retrieves a batch of inputs and targets from the data reader. The RNN then performs a forward pass on the input sequence and computes the output probabilities. The backward pass is performed using the targets and output probabilities to calculate the gradients of the parameters of the network. The Adagrad algorithm is used to update the weights of the network.**\n",
    "\n",
    "**The method then calculates and updates the loss using the updated weights. The previous hidden state is updated for the next batch. The method prints the progress every 500 iterations by generating a sample of text using the sample method and printing the loss.**\n",
    "\n",
    "\n",
    "**The train method can be summarized by the following steps:**\n",
    "\n",
    " \n",
    "**$1.$ Initialize $a_{prev}$ to zero at the beginning of each sequence.**\n",
    "\n",
    "**$2.$ Retrieve a batch of inputs and targets from the data reader.**\n",
    "\n",
    "**$3.$ Perform a forward pass on the input sequence and compute the output probabilities.**\n",
    "\n",
    "**$4.$ Perform a backward pass using the targets and output probabilities to calculate the gradients of the parameters of the network.**\n",
    "\n",
    "**$5.$ Use the Adagrad algorithm to update the weights of the network.**\n",
    "\n",
    "**$6.$ Calculate and update the loss using the updated weights.**\n",
    "\n",
    "**$7.$ Update the previous hidden state for the next batch.**\n",
    "\n",
    "**$8.$ Print progress every 10000 iterations by generating a sample of text using the sample method and printing the loss.**\n",
    "\n",
    "**$9.$ Repeat steps $2$-$8$ until the smooth loss is less than the threshold value.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, vocab_size, hidden_size=100, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        \n",
    "        self.Wax = np.random.randn(hidden_size, vocab_size) * 0.01\n",
    "        self.Waa = np.random.randn(hidden_size, hidden_size) * 0.01\n",
    "        self.Wya = np.random.randn(vocab_size, hidden_size) * 0.01\n",
    "        self.ba = np.zeros((hidden_size, 1))\n",
    "        self.by = np.zeros((vocab_size, 1))\n",
    "        \n",
    "        self.mWax, self.vWax = np.zeros_like(self.Wax), np.zeros_like(self.Wax)\n",
    "        self.mWaa, self.vWaa = np.zeros_like(self.Waa), np.zeros_like(self.Waa)\n",
    "        self.mWya, self.vWya = np.zeros_like(self.Wya), np.zeros_like(self.Wya)\n",
    "        self.mba, self.vba = np.zeros_like(self.ba), np.zeros_like(self.ba)\n",
    "        self.mby, self.vby = np.zeros_like(self.by), np.zeros_like(self.by)\n",
    "        \n",
    "    def forward(self, X, a_prev):\n",
    "        a_next = np.tanh(np.dot(self.Wax, X) + np.dot(self.Waa, a_prev) + self.ba)\n",
    "        y_pred = np.exp(np.dot(self.Wya, a_next) + self.by) / np.sum(np.exp(np.dot(self.Wya, a_next) + self.by))\n",
    "        return a_next, y_pred\n",
    "    \n",
    "    def backward(self, X, a, y_preds, targets):\n",
    "        dWax, dWaa, dWya = np.zeros_like(self.Wax), np.zeros_like(self.Waa), np.zeros_like(self.Wya)\n",
    "        dba, dby = np.zeros_like(self.ba), np.zeros_like(self.by)\n",
    "        \n",
    "        dy = y_preds - targets\n",
    "        dWya = np.dot(dy, a.T)\n",
    "        dby = dy\n",
    "        da = np.dot(self.Wya.T, dy)\n",
    "        \n",
    "        dtanh = (1 - a ** 2) * da\n",
    "        dWax = np.dot(dtanh, X.T)\n",
    "        dWaa = np.dot(dtanh, a.T)\n",
    "        dba = np.sum(dtanh, axis=1, keepdims=True)\n",
    "        \n",
    "        for dparam in [dWax, dWaa, dWya, dba, dby]:\n",
    "            np.clip(dparam, -1, 1, out=dparam)\n",
    "        \n",
    "        return dWax, dWaa, dWya, dba, dby\n",
    "    \n",
    "    def adamW(self, dWax, dWaa, dWya, dba, dby, t):\n",
    "        for param, dparam, m, v in zip(\n",
    "            [self.Wax, self.Waa, self.Wya, self.ba, self.by],\n",
    "            [dWax, dWaa, dWya, dba, dby],\n",
    "            [self.mWax, self.mWaa, self.mWya, self.mba, self.mby],\n",
    "            [self.vWax, self.vWaa, self.vWya, self.vba, self.vby]\n",
    "        ):\n",
    "            m[:] = self.beta1 * m + (1 - self.beta1) * dparam\n",
    "            v[:] = self.beta2 * v + (1 - self.beta2) * (dparam ** 2)\n",
    "            m_hat = m / (1 - self.beta1 ** t)\n",
    "            v_hat = v / (1 - self.beta2 ** t)\n",
    "            param -= self.learning_rate * m_hat / (np.sqrt(v_hat) + self.epsilon)\n",
    "    \n",
    "    def loss(self, y_pred, y_true):\n",
    "        return -np.sum(y_true * np.log(y_pred))\n",
    "    \n",
    "    def train(self, X, targets, num_iterations=1000):\n",
    "        a_prev = np.zeros((self.hidden_size, 1))\n",
    "        smooth_loss = -np.log(1.0 / self.vocab_size) * len(X)\n",
    "        \n",
    "        for t in range(1, num_iterations + 1):\n",
    "            a, y_pred = self.forward(X, a_prev)\n",
    "            loss = self.loss(y_pred, targets)\n",
    "            \n",
    "            dWax, dWaa, dWya, dba, dby = self.backward(X, a, y_pred, targets)\n",
    "            self.adamW(dWax, dWaa, dWya, dba, dby, t)\n",
    "            \n",
    "            smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "            if t % 100 == 0:\n",
    "                print(f'Iteration {t}, Smooth Loss: {smooth_loss:.4f}')\n",
    "    \n",
    "    def predict(self, start, char_to_index, index_to_char, length=100):\n",
    "        a = np.zeros((self.hidden_size, 1))\n",
    "        X = np.zeros((self.vocab_size, 1))\n",
    "        chars = list(start)\n",
    "        \n",
    "        for ch in start:\n",
    "            X[char_to_index[ch]] = 1\n",
    "            a, _ = self.forward(X, a)\n",
    "        \n",
    "        for _ in range(length):\n",
    "            _, y_pred = self.forward(X, a)\n",
    "            idx = np.random.choice(range(self.vocab_size), p=y_pred.ravel())\n",
    "            chars.append(index_to_char[idx])\n",
    "            X = np.zeros((self.vocab_size, 1))\n",
    "            X[idx] = 1\n",
    "        \n",
    "        return ''.join(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'DataGenerator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Initialize data generator with dinosaur names file\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data_generator \u001b[38;5;241m=\u001b[39m \u001b[43mDataGenerator\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../MIT_6.S191/dinos.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Initialize RNN model with required parameters\u001b[39;00m\n\u001b[1;32m      5\u001b[0m rnn \u001b[38;5;241m=\u001b[39m RNN(\n\u001b[1;32m      6\u001b[0m     hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m,          \u001b[38;5;66;03m# Size of hidden state\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     data_generator\u001b[38;5;241m=\u001b[39mdata_generator,  \u001b[38;5;66;03m# Data generator instance\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     sequence_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m25\u001b[39m,       \u001b[38;5;66;03m# Length of input sequences\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m        \u001b[38;5;66;03m# Learning rate for optimization\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'DataGenerator' is not defined"
     ]
    }
   ],
   "source": [
    "# Initialize data generator with dinosaur names file\n",
    "data_generator = DataGenerator('../MIT_6.S191/dinos.txt')\n",
    "\n",
    "# Initialize RNN model with required parameters\n",
    "rnn = RNN(\n",
    "    hidden_size=200,          # Size of hidden state\n",
    "    data_generator=data_generator,  # Data generator instance\n",
    "    sequence_length=25,       # Length of input sequences\n",
    "    learning_rate=1e-3        # Learning rate for optimization\n",
    ")\n",
    "\n",
    "# Train the RNN model\n",
    "rnn.train(generated_names=5)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
